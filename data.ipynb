{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import string\n",
    "mbti = pd.read_csv('../Downloads/mbti_1.csv')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.texts, self.labels = zip(*data)\n",
    "        unique = set(self.labels)\n",
    "        self.labelmap = dict(zip(unique, range(len(unique))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.texts[idx], self.labels[idx]\n",
    "        return self.labelmap[label], sentence  # Keep sentence as a string or tokenize output\n",
    "\n",
    "def clean_sentence(sen: str) -> str:\n",
    "    sen = ' '.join(word for word in sen.split() if 'http' not in word)\n",
    "    return sen.strip().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "data = []\n",
    "for _, (typ, sens) in mbti.iterrows():\n",
    "    data += [(sen, typ) for sen in map(clean_sentence, sens.split('|||')) if len(sen) > 10]\n",
    "data = MBTIDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn \n",
    "import torch \n",
    "from typing import List \n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "class MBTIModel(nn.Module):\n",
    "    def __init__(self, pretrained: str, num_classes: int, device: str):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(pretrained, return_dict=False)\n",
    "        self.tkz = AutoTokenizer.from_pretrained(pretrained)\n",
    "        self.dim = AutoConfig.from_pretrained(pretrained).hidden_size\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.dim, num_classes),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "        self.device = device \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, texts: List[str]) -> torch.Tensor:\n",
    "        x = self.tkz(texts, padding=True, truncation=True, padding_side='right', return_tensors='pt').to(self.device)\n",
    "        x = self.model(input_ids=x.input_ids, attention_mask=x.attention_mask)[1][:, 0]\n",
    "        return self.ffn(x)\n",
    "\n",
    "\n",
    "    def loss(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        return self.criteria(logits, labels)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval(self, texts: List[str], labels: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.forward(texts)\n",
    "        return (labels == logits).sum()/len(texts)\n",
    "\n",
    "model = MBTIModel(pretrained='bert-base-uncased', num_classes=16, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "train_ds = DataLoader(data, batch_size=100, shuffle=True)\n",
    "opt = AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    for texts, labels in train_ds:\n",
    "        labels = torch.tensor(labels)\n",
    "        loss = model(texts)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackudc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
